{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Lab 2: Classification</center></h1>\n",
    "<h3><center>A Deeper Analysis of Covid-19 Data</center></h3>\n",
    "<p><center>DS 7331</center></p>\n",
    "<p><center>Created by Sadik Aman, Dawn Bowerman, Zachary Harris, Alexandre Jasserme</center></p>\n",
    "\n",
    "<p><center>Sections of this code was adapted from: \n",
    "    <li>https://github.com/jakemdrew/DataMiningNotebooks</li>\n",
    "    <li> https://scikit-learn.org/stable/auto_examples/linear_model/plot_theilsen.html</li>\n",
    "    <li> https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Part 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Define and prepare your class variables. Use proper variable representations\n",
    "(int, float, one-hot, etc.).\n",
    "Use pre-processing methods (as needed) for dimensionality\n",
    "reduction, scaling, etc. \n",
    "Remove variables that are not needed/useful for the analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34965 entries, 0 to 34964\n",
      "Data columns (total 26 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   continent                       34965 non-null  object \n",
      " 1   location                        34965 non-null  object \n",
      " 2   date                            34965 non-null  object \n",
      " 3   new_cases                       34965 non-null  float64\n",
      " 4   new_cases_smoothed              34965 non-null  float64\n",
      " 5   new_deaths                      34965 non-null  float64\n",
      " 6   new_deaths_smoothed             34965 non-null  float64\n",
      " 7   reproduction_rate               34965 non-null  float64\n",
      " 8   new_vaccinations_smoothed       34965 non-null  float64\n",
      " 9   new_people_vaccinated_smoothed  34965 non-null  float64\n",
      " 10  stringency_index                34965 non-null  float64\n",
      " 11  population                      34965 non-null  int64  \n",
      " 12  population_density              34965 non-null  float64\n",
      " 13  median_age                      34965 non-null  float64\n",
      " 14  aged_65_older                   34965 non-null  float64\n",
      " 15  aged_70_older                   34965 non-null  float64\n",
      " 16  gdp_per_capita                  34965 non-null  float64\n",
      " 17  cardiovasc_death_rate           34965 non-null  float64\n",
      " 18  diabetes_prevalence             34965 non-null  float64\n",
      " 19  handwashing_facilities          34965 non-null  float64\n",
      " 20  hospital_beds_per_thousand      34965 non-null  float64\n",
      " 21  life_expectancy                 34965 non-null  float64\n",
      " 22  human_development_index         34965 non-null  float64\n",
      " 23  stringency_range                34944 non-null  object \n",
      " 24  new_cases_range                 34965 non-null  object \n",
      " 25  new_deaths_range                34965 non-null  object \n",
      "dtypes: float64(19), int64(1), object(6)\n",
      "memory usage: 6.9+ MB\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "#Libraries\n",
    "import plotly\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import time\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# run logistic regression and vary some parameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer, mean_squared_error\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression, TheilSenRegressor\n",
    "from sklearn.linear_model import RANSACRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in the CSV file and displaying the resulting dataframe\n",
    "df = pd.read_csv('Data/owid-covid-data_modified.csv') \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 34965 entries, 0 to 34964\n",
      "Data columns (total 26 columns):\n",
      " #   Column                          Non-Null Count  Dtype         \n",
      "---  ------                          --------------  -----         \n",
      " 0   continent                       34965 non-null  object        \n",
      " 1   location                        34965 non-null  object        \n",
      " 2   date                            34965 non-null  datetime64[ns]\n",
      " 3   new_cases                       34965 non-null  float64       \n",
      " 4   new_cases_smoothed              34965 non-null  float64       \n",
      " 5   new_deaths                      34965 non-null  float64       \n",
      " 6   new_deaths_smoothed             34965 non-null  float64       \n",
      " 7   reproduction_rate               34965 non-null  float64       \n",
      " 8   new_vaccinations_smoothed       34965 non-null  float64       \n",
      " 9   new_people_vaccinated_smoothed  34965 non-null  float64       \n",
      " 10  stringency_index                34965 non-null  float64       \n",
      " 11  population                      34965 non-null  int64         \n",
      " 12  population_density              34965 non-null  float64       \n",
      " 13  median_age                      34965 non-null  float64       \n",
      " 14  aged_65_older                   34965 non-null  float64       \n",
      " 15  aged_70_older                   34965 non-null  float64       \n",
      " 16  gdp_per_capita                  34965 non-null  float64       \n",
      " 17  cardiovasc_death_rate           34965 non-null  float64       \n",
      " 18  diabetes_prevalence             34965 non-null  float64       \n",
      " 19  handwashing_facilities          34965 non-null  float64       \n",
      " 20  hospital_beds_per_thousand      34965 non-null  float64       \n",
      " 21  life_expectancy                 34965 non-null  float64       \n",
      " 22  human_development_index         34965 non-null  float64       \n",
      " 23  stringency_range                34944 non-null  object        \n",
      " 24  new_cases_range                 34965 non-null  object        \n",
      " 25  new_deaths_range                34965 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(19), int64(1), object(5)\n",
      "memory usage: 7.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Ideas from https://scikit-learn.org/stable/modules/cross_validation.html#timeseries-cv\n",
    "\n",
    "# Sorting data frame by date column\n",
    "df['date'] = pd.to_datetime(df['date']) # Converting data columnn to datetime\n",
    "\n",
    "df = df.sort_values(by='date', ascending=True)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 34965 entries, 0 to 34964\n",
      "Data columns (total 15 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   location                    34965 non-null  object \n",
      " 1   new_deaths                  34965 non-null  float64\n",
      " 2   reproduction_rate           34965 non-null  float64\n",
      " 3   new_vaccinations_smoothed   34965 non-null  float64\n",
      " 4   stringency_index            34965 non-null  float64\n",
      " 5   population                  34965 non-null  int64  \n",
      " 6   median_age                  34965 non-null  float64\n",
      " 7   aged_65_older               34965 non-null  float64\n",
      " 8   gdp_per_capita              34965 non-null  float64\n",
      " 9   diabetes_prevalence         34965 non-null  float64\n",
      " 10  handwashing_facilities      34965 non-null  float64\n",
      " 11  hospital_beds_per_thousand  34965 non-null  float64\n",
      " 12  life_expectancy             34965 non-null  float64\n",
      " 13  human_development_index     34965 non-null  float64\n",
      " 14  stringency_range            34944 non-null  object \n",
      "dtypes: float64(12), int64(1), object(2)\n",
      "memory usage: 4.3+ MB\n"
     ]
    }
   ],
   "source": [
    "covid_df = df.drop([\"new_deaths_smoothed\",\n",
    "                    \"new_cases\",\n",
    "                    \"new_cases_smoothed\",\n",
    "                    \"continent\",\n",
    "                    #\"location\",\n",
    "                    #\"stringency_range\",\n",
    "                    \"new_cases_range\",\n",
    "                    \"date\",\n",
    "                    \"new_people_vaccinated_smoothed\",\n",
    "                    \"population_density\",\n",
    "                    \"aged_70_older\",\n",
    "                    \"cardiovasc_death_rate\",\n",
    "                    \"new_deaths_range\"], axis=1)\n",
    "\n",
    "covid_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset: (34965, 15)\n",
      "Number of unique classes: 34965\n"
     ]
    }
   ],
   "source": [
    "print ('Size of the dataset:', covid_df.shape)\n",
    "print ('Number of unique classes:', len(covid_df.new_deaths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Data Preparation Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Modeling and Evaluation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Choose and explain your evaluation metrics that you will use (i.e., accuracy,precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We have selected the Root Mean Squared Error (RMSE) for our evaluation metric.  The RMSE measures the average error produced by the model while predicting the result for a study. The RMSE is the square root of the mean squared error (MSE); the average squared difference between the observed actual results and the results predicted by the model. The MSE = mean((observed - predicted)^2) and the RMSE = sqrt(MSE). A model with a lower RMSE is a better model.  We believe that this metric is appropriate because RMSE is a mathematical equation that equates the error between the predicted and the actual results to determine the correctness of a model.  Since most of our data is numerical it is a likely choice as metrics for binary data will not work here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Modeling and Evaluation 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Choose the method you will use for dividing your data into training and testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Modeling and Evaluation 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We have selected RANSAC Regression, Theil-sen Regression, and Support Vector Machine models to analyze our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Model Using Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "len(np.unique(covid_df.location))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Theil-sen Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Afghanistan' 0.98 14115.0 ... 64.83 0.511 '40-50']\n",
      " ['Bermuda' 0.323548387 430.0 ... 82.59 0.545586022 '30-40']\n",
      " ['Guatemala' 1.16 8217.0 ... 74.3 0.663 '50-60']\n",
      " ...\n",
      " ['Thailand' 1.18 64643.66667 ... 77.15 0.777 '40-50']\n",
      " ['Qatar' 2.23 7892.0 ... 80.23 0.848 '30-40']\n",
      " ['Zimbabwe' 0.8 8788.0 ... 61.49 0.571 '50-60']]\n",
      "[91.  0. 67. ... 10.  0. 13.]\n",
      "0         91.0\n",
      "3515       0.0\n",
      "13690     67.0\n",
      "21275    201.0\n",
      "31265      0.0\n",
      "         ...  \n",
      "14429      0.0\n",
      "3144       0.0\n",
      "31819     10.0\n",
      "26269      0.0\n",
      "34964     13.0\n",
      "Name: new_deaths, Length: 34965, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## KFold cross-validation   **** I think this should be moved.\n",
    "\n",
    "#separate the predicting attribute into Y for model training\n",
    "covid_target = covid_df.new_deaths\n",
    "\n",
    "#separate the other attributes from the predicting attribute\n",
    "covid_df = covid_df.drop(\"new_deaths\",axis=1)\n",
    "\n",
    "## Define variables for the for loop\n",
    "kf = KFold(n_splits=10)\n",
    "RMSE_sum=0\n",
    "RMSE_length=10\n",
    "X = np.array(covid_df)\n",
    "y = np.array(covid_target)\n",
    "print(X)\n",
    "print(y)\n",
    "print(covid_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Uzbekistan'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-347455a44a48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m## Fit the Linear Regression Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mlr_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_X_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_y_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;31m## Compute the predictions for the test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0maccept_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositive\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coo\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    663\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m    965\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Uzbekistan'"
     ]
    }
   ],
   "source": [
    "  for loop_number, (train, test) in enumerate(kf.split(X)):\n",
    "    \n",
    "    \n",
    "    ## Get Training Matrix and Vector\n",
    "\n",
    "    training_X_array = X[train]\n",
    "    training_y_array = y[train]\n",
    "\n",
    "    ## Get Testing Matrix Values\n",
    "\n",
    "    X_test_array = X[test]\n",
    "    y_actual_values = y[test]\n",
    "\n",
    "    ## Fit the Linear Regression Model\n",
    "\n",
    "    lr_model = LinearRegression().fit(training_X_array, training_y_array)\n",
    "\n",
    "    ## Compute the predictions for the test data\n",
    "\n",
    "    prediction = lr_model.predict(X_test_array)      \n",
    "    deaths_probabilites = np.array(prediction)   \n",
    "\n",
    "    ## Calculate the RMSE\n",
    "\n",
    "    RMSE_cross_fold = mean_squared_error(deaths_probabilites, y_actual_values)\n",
    "\n",
    "    ## Add each RMSE_cross_fold value to the sum\n",
    "\n",
    "    RMSE_sum=RMSE_cross_fold+RMSE_sum\n",
    "\n",
    "## Calculate the average and print    \n",
    "\n",
    "RMSE_cross_fold_avg=RMSE_sum/RMSE_length\n",
    "\n",
    "print('The Mean RMSE across all folds is',RMSE_cross_fold_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Modeling and Evaluation 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Modeling and Evaluation 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques—be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Modeling and Evaluation 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How would you measure the model's value if it was used by these parties? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How would your deploy your model for interested parties? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What other data should be collected? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How often would the model need to be updated, etc.? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Exceptional Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You have free reign to provide additional analyses. One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
