{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Lab 2: Classification</center></h1>\n",
    "<h3><center>A Deeper Analysis of Covid-19 Data</center></h3>\n",
    "<p><center>DS 7331</center></p>\n",
    "<p><center>Created by Sadik Aman, Dawn Bowerman, Zachary Harris, Alexandre Jasserme</center></p>\n",
    "\n",
    "<p><center>Sections of this code was adapted from: \n",
    "    <li>https://github.com/jakemdrew/DataMiningNotebooks</li>\n",
    "    <li> https://scikit-learn.org/stable/auto_examples/linear_model/plot_theilsen.html</li>\n",
    "    <li> https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Part 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Define and prepare your class variables. Use proper variable representations\n",
    "(int, float, one-hot, etc.).\n",
    "Use pre-processing methods (as needed) for dimensionality\n",
    "reduction, scaling, etc. \n",
    "Remove variables that are not needed/useful for the analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "#Libraries\n",
    "import plotly\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import time\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# run logistic regression and vary some parameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer, mean_squared_error\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression, TheilSenRegressor\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34965 entries, 0 to 34964\n",
      "Data columns (total 26 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   continent                       34965 non-null  object \n",
      " 1   location                        34965 non-null  object \n",
      " 2   date                            34965 non-null  object \n",
      " 3   new_cases                       34965 non-null  float64\n",
      " 4   new_cases_smoothed              34965 non-null  float64\n",
      " 5   new_deaths                      34965 non-null  float64\n",
      " 6   new_deaths_smoothed             34965 non-null  float64\n",
      " 7   reproduction_rate               34965 non-null  float64\n",
      " 8   new_vaccinations_smoothed       34965 non-null  float64\n",
      " 9   new_people_vaccinated_smoothed  34965 non-null  float64\n",
      " 10  stringency_index                34965 non-null  float64\n",
      " 11  population                      34965 non-null  int64  \n",
      " 12  population_density              34965 non-null  float64\n",
      " 13  median_age                      34965 non-null  float64\n",
      " 14  aged_65_older                   34965 non-null  float64\n",
      " 15  aged_70_older                   34965 non-null  float64\n",
      " 16  gdp_per_capita                  34965 non-null  float64\n",
      " 17  cardiovasc_death_rate           34965 non-null  float64\n",
      " 18  diabetes_prevalence             34965 non-null  float64\n",
      " 19  handwashing_facilities          34965 non-null  float64\n",
      " 20  hospital_beds_per_thousand      34965 non-null  float64\n",
      " 21  life_expectancy                 34965 non-null  float64\n",
      " 22  human_development_index         34965 non-null  float64\n",
      " 23  stringency_range                34944 non-null  object \n",
      " 24  new_cases_range                 34965 non-null  object \n",
      " 25  new_deaths_range                34965 non-null  object \n",
      "dtypes: float64(19), int64(1), object(6)\n",
      "memory usage: 6.9+ MB\n"
     ]
    }
   ],
   "source": [
    "#Loading in the CSV file and displaying the resulting dataframe\n",
    "df = pd.read_csv('Data/owid-covid-data_modified.csv') \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 34965 entries, 0 to 34964\n",
      "Data columns (total 26 columns):\n",
      " #   Column                          Non-Null Count  Dtype         \n",
      "---  ------                          --------------  -----         \n",
      " 0   continent                       34965 non-null  object        \n",
      " 1   location                        34965 non-null  object        \n",
      " 2   date                            34965 non-null  datetime64[ns]\n",
      " 3   new_cases                       34965 non-null  float64       \n",
      " 4   new_cases_smoothed              34965 non-null  float64       \n",
      " 5   new_deaths                      34965 non-null  float64       \n",
      " 6   new_deaths_smoothed             34965 non-null  float64       \n",
      " 7   reproduction_rate               34965 non-null  float64       \n",
      " 8   new_vaccinations_smoothed       34965 non-null  float64       \n",
      " 9   new_people_vaccinated_smoothed  34965 non-null  float64       \n",
      " 10  stringency_index                34965 non-null  float64       \n",
      " 11  population                      34965 non-null  int64         \n",
      " 12  population_density              34965 non-null  float64       \n",
      " 13  median_age                      34965 non-null  float64       \n",
      " 14  aged_65_older                   34965 non-null  float64       \n",
      " 15  aged_70_older                   34965 non-null  float64       \n",
      " 16  gdp_per_capita                  34965 non-null  float64       \n",
      " 17  cardiovasc_death_rate           34965 non-null  float64       \n",
      " 18  diabetes_prevalence             34965 non-null  float64       \n",
      " 19  handwashing_facilities          34965 non-null  float64       \n",
      " 20  hospital_beds_per_thousand      34965 non-null  float64       \n",
      " 21  life_expectancy                 34965 non-null  float64       \n",
      " 22  human_development_index         34965 non-null  float64       \n",
      " 23  stringency_range                34944 non-null  object        \n",
      " 24  new_cases_range                 34965 non-null  object        \n",
      " 25  new_deaths_range                34965 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(19), int64(1), object(5)\n",
      "memory usage: 7.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Ideas from https://scikit-learn.org/stable/modules/cross_validation.html#timeseries-cv\n",
    "\n",
    "# Sorting data frame by date column\n",
    "df['date'] = pd.to_datetime(df['date']) # Converting data columnn to datetime\n",
    "\n",
    "df = df.sort_values(by='date', ascending=True)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 34965 entries, 0 to 34964\n",
      "Data columns (total 13 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   new_deaths                  34965 non-null  float64\n",
      " 1   reproduction_rate           34965 non-null  float64\n",
      " 2   new_vaccinations_smoothed   34965 non-null  float64\n",
      " 3   stringency_index            34965 non-null  float64\n",
      " 4   population                  34965 non-null  int64  \n",
      " 5   median_age                  34965 non-null  float64\n",
      " 6   aged_65_older               34965 non-null  float64\n",
      " 7   gdp_per_capita              34965 non-null  float64\n",
      " 8   diabetes_prevalence         34965 non-null  float64\n",
      " 9   handwashing_facilities      34965 non-null  float64\n",
      " 10  hospital_beds_per_thousand  34965 non-null  float64\n",
      " 11  life_expectancy             34965 non-null  float64\n",
      " 12  human_development_index     34965 non-null  float64\n",
      "dtypes: float64(12), int64(1)\n",
      "memory usage: 3.7 MB\n"
     ]
    }
   ],
   "source": [
    "covid_df = df.drop([\"new_deaths_smoothed\",\n",
    "                    \"new_cases\",\n",
    "                    \"new_cases_smoothed\",\n",
    "                    \"continent\",\n",
    "                    \"location\",\n",
    "                    \"stringency_range\",\n",
    "                    \"new_cases_range\",\n",
    "                    \"date\",\n",
    "                    \"new_people_vaccinated_smoothed\",\n",
    "                    \"population_density\",\n",
    "                    \"aged_70_older\",\n",
    "                    \"cardiovasc_death_rate\",\n",
    "                    \"new_deaths_range\"], axis=1)\n",
    "\n",
    "covid_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset: (34965, 13)\n",
      "Number of unique classes: 34965\n"
     ]
    }
   ],
   "source": [
    "print ('Size of the dataset:', covid_df.shape)\n",
    "print ('Number of unique classes:', len(covid_df.new_deaths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Data Preparation Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Modeling and Evaluation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Choose and explain your evaluation metrics that you will use (i.e., accuracy,precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We have selected the Root Mean Squared Error (RMSE) for our evaluation metric.  The RMSE measures the average error produced by the model while predicting the result for a study. The RMSE is the square root of the mean squared error (MSE); the average squared difference between the observed actual results and the results predicted by the model. The MSE = mean((observed - predicted)^2) and the RMSE = sqrt(MSE). A model with a lower RMSE is a better model.  We believe that this metric is appropriate because RMSE is a mathematical equation that equates the error between the predicted and the actual results to determine the correctness of a model.  Since most of our data is numerical it is a likely choice as metrics for binary data will not work here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Modeling and Evaluation 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Choose the method you will use for dividing your data into training and testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "\n",
    "cv_object = TimeSeriesSplit( n_splits=10)\n",
    "\n",
    "covid_target = covid_df.new_deaths\n",
    "\n",
    "\n",
    "#separate the other attributes from the predicting attribute\n",
    "covid_df = covid_df.drop(\"new_deaths\",axis=1)\n",
    "\n",
    "X = np.array(covid_df)\n",
    "y = np.array(covid_target)\n",
    "\n",
    "\n",
    "# TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None)\n",
    "for train_index, test_index in cv_object.split(X):\n",
    "   \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index] \n",
    "    \n",
    "    \n",
    "\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Modeling and Evaluation 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We have selected RANSAC Regression, Theil-sen Regression, and Support Vector Machine models to analyze our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Model Using Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-987ba5a750f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0msvm_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rbf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecision_function_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ovo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0msvm_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# train object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get test set predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    197\u001b[0m             )\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         sample_weight = np.asarray(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_validate_targets\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;34m\"multilabel-sequences\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     ]:\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'continuous'"
     ]
    }
   ],
   "source": [
    "# cross validation loop and set the training and testing variable for one single iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) \n",
    "# SVMs on the data and play with the parameters and kernels\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train the model\n",
    "svm_clf = SVC(C=0.5, kernel='rbf', decision_function_shape='ovo', degree=3, gamma='auto') # get object\n",
    "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test_scaled) # get test set predictions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat, labels = [\"high\",\"medium\",\"low\"])\n",
    "print('accuracy:', acc )\n",
    "print(conf)\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )\n",
    "\n",
    "# make a dataframe of the training data\n",
    "df_tested_on = float_df.iloc[train_indices].copy() # saved from above, the indices chosen for training\n",
    "# get the support vectors from the trained model\n",
    "df_support = df_tested_on.iloc[svm_clf.support_,:].copy()\n",
    "\n",
    "df_support['new_deaths_range'] = y[svm_clf.support_] # add back in the 'new_deaths_range' Column to the pandas dataframe\n",
    "float_df['new_deaths_range'] = y # also add it back in for the original data\n",
    "df_support.info()\n",
    "\n",
    "# statistics of the attributes\n",
    "from pandas.plotting import boxplot\n",
    "\n",
    "# group the original data and the support vectors\n",
    "df_grouped_support = df_support.groupby(['new_deaths_range'])\n",
    "df_grouped = float_df.groupby(['new_deaths_range'])\n",
    "\n",
    "# plot KDE of Different variables\n",
    "vars_to_plot = ['stringency_index','population','gdp_per_capita','human_development_index']\n",
    "\n",
    "for v in vars_to_plot:\n",
    "    plt.figure(figsize=(20,4))\n",
    "    # plot support vector stats\n",
    "    plt.subplot(1,2,1)\n",
    "    ax = df_grouped_support[v].plot.kde() \n",
    "    plt.legend(['low','medium','high'])\n",
    "    plt.title(v+' (Instances chosen as Support Vectors)')\n",
    "    \n",
    "    # plot original distributions\n",
    "    plt.subplot(1,2,2)\n",
    "    ax = df_grouped[v].plot.kde() \n",
    "    plt.legend(['low','medium','high'])\n",
    "    plt.title(v+' (Original)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Theil-sen Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KFold cross-validation   **** I think this should be moved.\n",
    "\n",
    "#separate the predicting attribute into Y for model training\n",
    "covid_target = covid_df.new_deaths\n",
    "\n",
    "#separate the other attributes from the predicting attribute\n",
    "covid_df = covid_df.drop(\"new_deaths\",axis=1)\n",
    "\n",
    "## Define variables for the for loop\n",
    "kf = KFold(n_splits=10)\n",
    "RMSE_sum=0\n",
    "RMSE_length=10\n",
    "X = np.array(covid_df)\n",
    "y = np.array(covid_target)\n",
    "print(X)\n",
    "print(y)\n",
    "print(covid_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  for loop_number, (train, test) in enumerate(kf.split(X)):\n",
    "    \n",
    "    \n",
    "    ## Get Training Matrix and Vector\n",
    "\n",
    "    training_X_array = X[train]\n",
    "    training_y_array = y[train]\n",
    "\n",
    "    ## Get Testing Matrix Values\n",
    "\n",
    "    X_test_array = X[test]\n",
    "    y_actual_values = y[test]\n",
    "\n",
    "    ## Fit the Linear Regression Model\n",
    "\n",
    "    lr_model = LinearRegression().fit(training_X_array, training_y_array)\n",
    "\n",
    "    ## Compute the predictions for the test data\n",
    "\n",
    "    prediction = lr_model.predict(X_test_array)      \n",
    "    deaths_probabilites = np.array(prediction)   \n",
    "\n",
    "    ## Calculate the RMSE\n",
    "\n",
    "    RMSE_cross_fold = mean_squared_error(deaths_probabilites, y_actual_values)\n",
    "\n",
    "    ## Add each RMSE_cross_fold value to the sum\n",
    "\n",
    "    RMSE_sum=RMSE_cross_fold+RMSE_sum\n",
    "\n",
    "## Calculate the average and print    \n",
    "\n",
    "RMSE_cross_fold_avg=RMSE_sum/RMSE_length\n",
    "\n",
    "print('The Mean RMSE across all folds is',RMSE_cross_fold_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Modeling and Evaluation 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Modeling and Evaluation 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniquesâ€”be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Modeling and Evaluation 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How would you measure the model's value if it was used by these parties? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How would your deploy your model for interested parties? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What other data should be collected? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How often would the model need to be updated, etc.? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Exceptional Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You have free reign to provide additional analyses. One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
